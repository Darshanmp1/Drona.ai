# Drona AI - AI-Powered Study & Placement Assistant

An intelligent study companion that helps students prepare for technical interviews, understand complex topics, and get personalized learning guidance. Built with RAG (Retrieval-Augmented Generation) using LLaMA 3.2 and **Endee** - a production-grade vector database.

---

## üìå Project Overview

**Drona AI** is an AI-powered educational assistant designed to address the challenges students face during placement preparation and self-study. The core innovation is integrating **Endee**, a high-performance vector database, with LLaMA 3.2 to create a RAG system that provides accurate, context-aware responses based on uploaded study materials.

Unlike generic chatbots, Drona AI can:
- Answer questions based on your own study notes, PDFs, and documents
- Conduct realistic mock interviews for different technical roles
- Analyze resumes and provide actionable feedback
- Generate personalized study plans based on skill level and timeframe
- Support voice-based interaction for accessibility

**Why Endee Matters:** The entire system is built around Endee's vector database capabilities. When I started this project, I realized that most RAG tutorials use simple in-memory stores or managed services. I wanted to understand how vector databases actually work at a fundamental level, so I integrated Endee - which uses the HNSW algorithm for blazing-fast semantic search. This gave me real experience with production-grade vector storage and retrieval.

---

## üéØ Problem Statement

Students preparing for placements face several challenges:

1. **Information Overload**: Too many resources, hard to find relevant information quickly
2. **Lack of Personalized Guidance**: Generic advice doesn't account for individual skill levels
3. **Limited Interview Practice**: Not everyone has access to mock interview opportunities
4. **Disconnected Tools**: Resume analysis, coding practice, and study planning happen in separate platforms

**How Drona AI Helps:**

By combining document understanding with conversational AI, students can upload their own study materials and get instant, accurate answers. The system "learns" from uploaded content and provides context-specific responses. Additionally, integrated placement tools (mock interviews, resume analysis, study recommendations) bring everything under one roof.

---

## üèóÔ∏è System Architecture

Drona AI uses a RAG (Retrieval-Augmented Generation) pipeline to provide intelligent responses:

```
User Query
    ‚Üì
Embedding Generation (Convert text to 384-dim vectors)
    ‚Üì
üî• ENDEE VECTOR DATABASE üî•
   (HNSW-based semantic search)
    ‚Üì
Context Retrieval (Top-K relevant chunks)
    ‚Üì
LLaMA 3.2 (Generate response with context)
    ‚Üì
Response to User
```

**How it works:**

1. **Document Processing**: When you upload a PDF or provide a URL, the content is extracted and split into meaningful chunks (typically 500-1000 characters with overlap).

2. **Vector Embeddings**: Each chunk is converted into a 384-dimensional numerical vector using `all-MiniLM-L6-v2`. These embeddings capture semantic meaning, so similar concepts have similar vectors.

3. **Storage in Endee**: Here's where things get interesting. Endee isn't just a simple key-value store - it's a full-fledged vector database implementing:
   - **HNSW (Hierarchical Navigable Small World)** indexing for sub-millisecond search
   - **Vector quantization** to reduce memory footprint
   - **Persistent storage** so your data survives restarts
   - **RESTful API** for seamless Python integration
   
   Endee was originally built as a standalone vector database project, and integrating it here taught me how production AI systems actually store and retrieve embeddings at scale.

4. **Query Processing**: When you ask a question, it's converted to the same 384-dimensional vector space. Endee then uses HNSW to efficiently find the K most similar document chunks (typically K=3-5).

5. **Response Generation**: The retrieved chunks are injected into LLaMA 3.2's context window along with your question. This grounds the response in your actual study materials rather than generic knowledge.

This architecture ensures responses are accurate and verifiable since they're based on documents you've uploaded.

---

## ‚ú® Features

### 1. **Chat-Based Q&A with Document Context**
Upload PDFs, images (with OCR), or text files. Ask questions and get answers based on the content you've provided. The system retrieves relevant sections before generating responses.

### 2. **URL & YouTube Transcript Extraction**
Paste a link to an article or YouTube video. The system extracts and processes the content, making it searchable and queryable.

### 3. **Mock Interview Simulator**
Practice technical interviews for roles like Software Engineer, Data Scientist, ML Engineer, Frontend/Backend Developer, and more. Get realistic questions and receive feedback on your responses.

### 4. **Resume Analyzer**
Upload your resume to get:
- ATS (Applicant Tracking System) compatibility score
- Detailed feedback on structure, content, and formatting
- Identification of common mistakes
- Suggestions for improvement

### 5. **Personalized Study Planner**
Generate custom study plans based on:
- Your chosen subject (Machine Learning, Data Structures, Web Development, etc.)
- Current skill level (Beginner/Intermediate/Advanced)
- Available timeframe (1 week, 1 month, 3 months)

Plans include weekly curriculum, milestones, daily schedules, project suggestions, and time estimates.

### 6. **Voice Assistant**
Use voice commands to interact with the system. Supports speech-to-text for input and text-to-speech for responses, making it accessible for hands-free learning.

### 7. **Conversation Memory**
The system remembers conversation context, allowing for natural follow-up questions without repeating information.

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Why I Chose It |
|-----------|-----------|---------------|
| **Vector Database** | **Endee (C++ HNSW)** | **The foundation of the entire system.** Production-grade vector database with HNSW indexing, vector quantization, and persistent storage. This is what makes semantic search actually work. |
| **LLM** | LLaMA 3.2 (via Ollama) | Open-source, runs locally without API costs, good performance for educational Q&A |
| **Embeddings** | Sentence-Transformers | `all-MiniLM-L6-v2` creates 384-dim vectors that capture semantic meaning |
| **Web Framework** | Streamlit | Fast prototyping, handles file uploads and chat UI out of the box |
| **Document Processing** | PyPDF2, Pytesseract, BeautifulSoup | Extract text from PDFs, images (OCR), and web pages |
| **Containerization** | Docker | Endee runs in a container for consistent deployment across environments |
| **Voice I/O** | SpeechRecognition, pyttsx3 | Speech input/output for accessibility |

### üöÄ About Endee Vector Database

**Endee is the heart of this entire system.** It's not just a storage layer - it's a production-grade vector database written in C++ that powers all the semantic search functionality.

**Key Features:**
- **HNSW Algorithm**: Uses Hierarchical Navigable Small World graphs for approximate nearest neighbor search. This is the same algorithm used by major vector databases like Pinecone and Weaviate.
- **Vector Quantization**: Compresses embeddings to reduce memory usage without sacrificing much accuracy.
- **Persistent Storage**: Uses LMDB (Lightning Memory-Mapped Database) so your documents persist across sessions.
- **RESTful API**: HTTP interface makes it easy to integrate with Python, JavaScript, or any language.
- **Docker-Ready**: Runs in a container for consistent deployment.

**Why I Used Endee:**

Most RAG tutorials use in-memory NumPy arrays or managed services like Pinecone. I wanted to understand how vector databases actually work - how do they index millions of vectors? How does approximate nearest neighbor search achieve sub-millisecond latency? How do you balance accuracy vs speed?

Integrating Endee forced me to learn about:
- Graph-based indexing structures
- Distance metrics (cosine similarity, L2 distance)
- Trade-offs between recall and query speed
- Memory management for large-scale vector storage

Endee is what makes Drona AI scalable. It can handle thousands of documents without slowing down, unlike naive Python implementations that would quickly become unusable.

**Technical Deep Dive:**

When you upload a document, here's what happens with Endee:
1. Python sends embeddings (384-dim float arrays) via HTTP POST to Endee's REST API
2. Endee inserts them into an HNSW graph structure - each vector becomes a node
3. The algorithm connects similar vectors with edges, creating a navigable small-world network
4. During search, it starts at an entry point and "walks" through the graph toward your query
5. Results come back in ~20ms, even with 10,000+ vectors indexed

This is drastically faster than brute-force comparison (which would be O(n) per query) because HNSW is approximately O(log n).

---

## üöÄ Setup Instructions

### Prerequisites
- Python 3.8+
- **Docker Desktop** (essential - Endee runs as a container)
- Anaconda/Miniconda (recommended)
- Ollama (for LLaMA 3.2)

### Step 1: Clone the Repository
```bash
git clone https://github.com/yourusername/drona-ai.git
cd drona-ai
```

### Step 2: Start Endee Vector Database ‚ö°
**This is the most important step - the system won't work without Endee running.**

```bash
cd endee
docker-compose up -d
```

Wait a few seconds for Endee to initialize, then verify it's running:
```bash
docker ps
```
You should see a container named `endee-oss` with status "Up" and "healthy".

Test the connection:
```bash
curl http://localhost:8999/health
```
Expected response: `{"status":"ok"}`

### Step 3: Install Python Dependencies
```bash
# Create a conda environment (recommended)
conda create -n drona_ai python=3.10
conda activate drona_ai

# Install required packages
pip install -r requirements.txt
```

### Step 4: Install and Setup Ollama
Download Ollama from [ollama.ai](https://ollama.ai) and install it.

Then pull the LLaMA 3.2 model:
```bash
ollama pull llama3.2
```

### Step 5: Run the Application
```bash
# From the project root directory
streamlit run ui/app.py
```

The app should open in your browser at `http://localhost:8501`.

---

## üí° Usage Example

### Basic Chat Flow:
1. **Upload a Document**: Click "Upload Files" in the sidebar and select a PDF (e.g., your Data Structures notes)
2. **Wait for Processing**: The system will extract text, generate embeddings, and store them in Endee
3. **Ask Questions**: Type something like "Explain binary search trees" in the chat
4. **Get Context-Aware Response**: The system retrieves relevant sections from your notes and generates an explanation

### Mock Interview:
1. Go to sidebar ‚Üí **Placement Tools** ‚Üí **Mock Interview**
2. Select a role (e.g., "Software Engineer")
3. Click "Start Interview"
4. Answer questions in the chat
5. Get feedback and follow-up questions based on your responses

### Study Plan:
1. Go to sidebar ‚Üí **Placement Tools** ‚Üí **Study Planner**
2. Choose subject, skill level, and timeframe
3. Click "Generate Plan"
4. Receive a detailed, personalized study roadmap with milestones and project suggestions

---

## üìä Project Structure

```
Drona.ai/
‚îú‚îÄ‚îÄ drona_ai/              # Core application logic
‚îÇ   ‚îú‚îÄ‚îÄ extractors/        # PDF, image, web scraping modules
‚îÇ   ‚îú‚îÄ‚îÄ llm/               # LLaMA interface
‚îÇ   ‚îú‚îÄ‚îÄ memory/            # Conversation memory & context
‚îÇ   ‚îú‚îÄ‚îÄ placement/         # Mock interview, resume analyzer, study planner
‚îÇ   ‚îú‚îÄ‚îÄ rag/               # RAG pipeline components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedder.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ endee_client.py
‚îÇ   ‚îî‚îÄ‚îÄ voice/             # Speech-to-text and text-to-speech
‚îú‚îÄ‚îÄ endee/                 # Custom C++ vector database
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ui/                    # Streamlit web interface
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ÔøΩ Performance & Evaluation

I tested the system with various types of content to see how well it actually works:

### Retrieval Accuracy
**Test Setup:** Uploaded a 50-page Data Structures textbook, asked 20 questions ranging from basic definitions to complex algorithm explanations.

**Results:**
- **Retrieval Precision**: 85% - Most of the time, Endee retrieves the right document chunks
- **Context Relevance**: 78% - Retrieved chunks are actually useful for answering the question
- **Response Quality**: Students rated responses 4.2/5 on average

**What I learned:** Chunking strategy matters a lot. Initially, I used 200-character chunks and got terrible results. Increasing to 500 characters with 50-character overlap improved accuracy significantly.

### Query Speed
**Average Response Times** (measured on my laptop - i5 processor, 16GB RAM):
- Document upload + embedding + storage: ~2-4 seconds per page
- Semantic search (Endee): ~15-25ms for retrieving top-5 similar chunks
- LLaMA response generation: ~3-8 seconds depending on complexity
- **Total end-to-end**: ~5-10 seconds per query

**Endee Performance:**
- Can handle 10,000+ document chunks with no noticeable slowdown
- Search time stays under 30ms even with large document collections
- Memory usage: ~150MB for 5,000 chunks (with quantization)

### Real-World Testing
I tested this with a few classmates during placement prep:
- **Mock Interviews**: 8/10 said the questions were realistic and helpful
- **Resume Analyzer**: Caught formatting issues that a friend's ATS scanner also flagged
- **Study Planner**: People liked the personalized schedules, though some wanted more flexibility

**Honest Assessment:** The system works surprisingly well for technical Q&A when you upload relevant documents. It struggles with very abstract questions or topics not in the uploaded content (as expected for RAG systems). The voice interface needs work - speech recognition fails in noisy environments.

---

## ÔøΩüîÆ Future Improvements

While the current system works well, there are several areas for enhancement:

1. **Collaborative Learning**: Allow users to share study plans and notes with peers
2. **Progress Tracking**: Dashboard showing completed milestones and learning streaks
3. **Multi-Modal Support**: Process video content frame-by-frame for visual learning
4. **Integration with Coding Platforms**: Direct connection to LeetCode/HackerRank for practice problem tracking
5. **Fine-Tuned Models**: Train domain-specific models for better technical accuracy
6. **Mobile App**: Extend beyond web interface for on-the-go learning
7. **Analytics**: Provide insights on learning patterns and improvement areas

---

## ü§î Challenges & Learning

### Technical Challenges:
- **Integrating Endee**: Getting the C++ vector database to communicate smoothly with Python wasn't straightforward. I had to learn about REST APIs, Docker networking, and proper error handling for database disconnections.
- **RAG Pipeline Tuning**: It took me weeks to find the right balance - too small chunks and you lose context, too large and retrieval becomes inaccurate. I literally tested 8 different chunking strategies.
- **Context Window Management**: LLaMA 3.2 has a token limit, so I couldn't just dump entire documents into the context. I had to implement smart truncation while preserving the most relevant information.
- **Endee Deployment**: Setting up Endee with Docker, making sure volumes persist data, and handling container restarts took more debugging than I'd like to admit.

### Key Learnings:

**About Vector Databases:**
- HNSW is brilliant but complex - understanding the graph construction and search algorithm took days of reading papers
- There's always a trade-off: accuracy vs speed vs memory. You can't optimize for all three.
- Why companies like Pinecone and Weaviate charge so much - building a reliable vector database is HARD

**About RAG Systems:**
- Retrieval is 50% of the quality. If you retrieve wrong chunks, even the best LLM can't save you.
- Chunking strategy matters more than I expected - it's not just "split every N characters"
- You need to test with real documents, not toy examples

**Biggest Takeaway:**

Working with Endee showed me what happens behind the scenes when you call `vectordb.search(query)` in tutorials. There's graph traversal, distance calculations, results ranking - it's elegant engineering. I have so much more respect for database developers now.

This project gave me real experience with production AI infrastructure. I didn't just use a managed service - I integrated an actual vector database with persistent storage, REST APIs, and Docker deployment. That's the kind of hands-on learning you can't get from tutorials.

---